{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "This notebook builds a neural network that can learn to play different games through reinforcement learning. The game will be simulated using [OpenAI Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:05:09,260] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's explore the environment\n",
    "# how many actions are possible?\n",
    "# will output discrete value 2 (you can just move the pole left or right)\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a random action\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "For this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, I'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, the Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network I will use an LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPoleController(object):\n",
    "    def __init__(self, n_input=4, n_hidden=10, n_output=1, initial_state=0.1, training_threshold=1.5):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.initial_state = initial_state\n",
    "        self.training_threshold = training_threshold\n",
    "        self.step_threshold = 0.5\n",
    "        # Action neural network\n",
    "        # Dense input -> (1 x n_input)\n",
    "        # LSTM -> (n_hidden)\n",
    "        # Dense output -> (n_output)\n",
    "        self.action_model = Sequential()\n",
    "        self.action_model.add(LSTM(self.n_hidden, input_shape=(1, self.n_input)))\n",
    "        self.action_model.add(Activation('tanh'))\n",
    "        self.action_model.add(Dense(self.n_output))\n",
    "        self.action_model.add(Activation('sigmoid'))\n",
    "        self.action_model.compile(loss='mse', optimizer='adam')\n",
    "        \n",
    "    def action(self, obs, prev_obs=None, prev_action=None):\n",
    "        x = np.ndarray(shape=(1, 1, self.n_input)).astype(K.floatx())\n",
    "        if prev_obs is not None:\n",
    "            prev_norm = np.linalg.norm(prev_obs)\n",
    "            if prev_norm > self.training_threshold:\n",
    "            # Compute a training step\n",
    "                x[0, 0, :] = prev_obs\n",
    "            if prev_norm < self.step_threshold:\n",
    "                y = np.array([prev_action]).astype(K.floatx())\n",
    "            else:\n",
    "                y = np.array([np.abs(prev_action - 1)]).astype(K.floatx())\n",
    "            self.action_model.train_on_batch(x, y)\n",
    "            # Predict new value\n",
    "            x[0, 0, :] = obs\n",
    "        output = self.action_model.predict(x, batch_size=1)\n",
    "        return self.step(output)\n",
    "    def step(self, value):\n",
    "        if value > self.step_threshold:\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of episodes\n",
    "nb_episodes = 100\n",
    "# Max execution time (in seconds)\n",
    "max_execution_time = 120\n",
    "# Set random seed\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:22,724] Making new env: CartPole-v0\n",
      "[2017-06-19 17:28:22,734] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1')\n",
      "[2017-06-19 17:28:22,741] Clearing 12 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI-Gym CartPole-v0 LSTM experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:23,167] Starting new video recorder writing to /Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1/openaigym.video.6.22823.video000000.mp4\n",
      "[2017-06-19 17:28:25,695] Starting new video recorder writing to /Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1/openaigym.video.6.22823.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 10 timesteps. Total reward: 9. Elapsed time: 2 s\n",
      "Episode 2 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 3 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 4 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 5 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 6 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 7 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:27,220] Starting new video recorder writing to /Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1/openaigym.video.6.22823.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 9 finished after 27 timesteps. Total reward: 26. Elapsed time: 0 s\n",
      "Episode 10 finished after 32 timesteps. Total reward: 31. Elapsed time: 0 s\n",
      "Episode 11 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 12 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 13 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 14 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 15 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 16 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 17 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 18 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 19 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 20 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 21 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 22 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 23 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 24 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 25 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 26 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:32,319] Starting new video recorder writing to /Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1/openaigym.video.6.22823.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 28 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 29 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 30 finished after 22 timesteps. Total reward: 21. Elapsed time: 0 s\n",
      "Episode 31 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 32 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 33 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 34 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 35 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 36 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 37 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 38 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 39 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 40 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 41 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 42 finished after 10 timesteps. Total reward: 9. Elapsed time: 0 s\n",
      "Episode 43 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 44 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 45 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 46 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 47 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 48 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 49 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 50 finished after 10 timesteps. Total reward: 9. Elapsed time: 0 s\n",
      "Episode 51 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 52 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 53 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 54 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 55 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 56 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 57 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 58 finished after 20 timesteps. Total reward: 19. Elapsed time: 0 s\n",
      "Episode 59 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 60 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 61 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 62 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 63 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:40,337] Starting new video recorder writing to /Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1/openaigym.video.6.22823.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 64 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 65 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 66 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 67 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 68 finished after 10 timesteps. Total reward: 9. Elapsed time: 0 s\n",
      "Episode 69 finished after 12 timesteps. Total reward: 11. Elapsed time: 0 s\n",
      "Episode 70 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 71 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 72 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 73 finished after 19 timesteps. Total reward: 18. Elapsed time: 0 s\n",
      "Episode 74 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 75 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 76 finished after 10 timesteps. Total reward: 9. Elapsed time: 0 s\n",
      "Episode 77 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 78 finished after 18 timesteps. Total reward: 17. Elapsed time: 0 s\n",
      "Episode 79 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 80 finished after 19 timesteps. Total reward: 18. Elapsed time: 0 s\n",
      "Episode 81 finished after 18 timesteps. Total reward: 17. Elapsed time: 0 s\n",
      "Episode 82 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 83 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 84 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 85 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 86 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 87 finished after 15 timesteps. Total reward: 14. Elapsed time: 0 s\n",
      "Episode 88 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 89 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 90 finished after 20 timesteps. Total reward: 19. Elapsed time: 0 s\n",
      "Episode 91 finished after 17 timesteps. Total reward: 16. Elapsed time: 0 s\n",
      "Episode 92 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 93 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 94 finished after 16 timesteps. Total reward: 15. Elapsed time: 0 s\n",
      "Episode 95 finished after 18 timesteps. Total reward: 17. Elapsed time: 0 s\n",
      "Episode 96 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n",
      "Episode 97 finished after 11 timesteps. Total reward: 10. Elapsed time: 0 s\n",
      "Episode 98 finished after 13 timesteps. Total reward: 12. Elapsed time: 0 s\n",
      "Episode 99 finished after 14 timesteps. Total reward: 13. Elapsed time: 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-19 17:28:48,971] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/bernhardmayr/Documents/Projekte/MLforTrading/QLearning/gym_deep_q_learning/cartpole_lstm-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 finished after 18 timesteps. Total reward: 17. Elapsed time: 0 s\n",
      "Average reward: 17.00\n"
     ]
    }
   ],
   "source": [
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "print('OpenAI-Gym CartPole-v0 LSTM experiment')\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, 'cartpole_lstm-1', force=True)\n",
    "cart_pole_controller = CartPoleController()\n",
    "total_reward = []\n",
    "\n",
    "for episode in range(nb_episodes):\n",
    "    # Reset environment\n",
    "    observation = env.reset()\n",
    "    previous_observation = observation\n",
    "    action = cart_pole_controller.action(observation)\n",
    "    previous_action = action\n",
    "    done = False\n",
    "    t = 0\n",
    "    partial_reward = 0.0\n",
    "    start_time = time.time()\n",
    "    elapsed_time = 0\n",
    "    \n",
    "    while not done and elapsed_time < max_execution_time:\n",
    "        t += 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        env.render()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        partial_reward += reward\n",
    "        action = cart_pole_controller.action(observation, previous_observation, previous_action)\n",
    "        previous_observation = observation\n",
    "        previous_action = action\n",
    "\n",
    "    print('Episode %d finished after %d timesteps. Total reward: %1.0f. Elapsed time: %d s' %\n",
    "          (episode+1, t+1, partial_reward, elapsed_time))\n",
    "\n",
    "env.close()\n",
    "total_reward.append(partial_reward)\n",
    "total_reward = np.array(total_reward)\n",
    "print('Average reward: %3.2f' % np.mean(total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
